D = {'func': 'networks.D_paper', 'dtype': 'float16'}
D_loss = {'func': 'loss.D_wgangp_acgan'}
D_opt = {'beta1': 0.0, 'beta2': 0.99, 'epsilon': 1e-08, 'use_loss_scaling': True}
EasyDict = <class 'config.EasyDict'>
G = {'func': 'networks.G_paper', 'dtype': 'float16', 'pixelnorm_epsilon': 0.0001}
G_loss = {'func': 'loss.G_wgan_acgan'}
G_opt = {'beta1': 0.0, 'beta2': 0.99, 'epsilon': 1e-08, 'use_loss_scaling': True}
data_dir = datasets
dataset = {'tfrecord_dir': 'celeba'}
desc = pgan-celeba-preset-v2-8gpus-fp16-GRAPH-HIST
env = {'TF_CPP_MIN_LOG_LEVEL': '1'}
grid = {'size': '1080p', 'layout': 'random'}
num_gpus = 8
random_seed = 1000
result_dir = results
sched = {'minibatch_base': 32, 'minibatch_dict': {4: 512, 8: 256, 16: 128, 32: 64, 64: 32}, 'G_lrate_dict': {128: 0.0015, 256: 0.002, 512: 0.003, 1024: 0.003}, 'D_lrate_dict': {128: 0.0015, 256: 0.002, 512: 0.003, 1024: 0.003}, 'max_minibatch_per_gpu': {512: 16, 1024: 8}}
tf_config = {'graph_options.place_pruned_graph': True, 'gpu_options.allow_growth': False}
train = {'func': 'train.train_progressive_gan', 'mirror_augment': True, 'total_kimg': 12000, 'save_tf_graph': True, 'save_weight_histograms': True}
